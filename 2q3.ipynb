{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nfrom torch import optim\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": "#          01234567\nsymbols = \"EBabcdXY\"\nsymbols_onehot = np.array([[1,0,0,0,0,0,0,0],\n                           [0,1,0,0,0,0,0,0],\n                           [0,0,1,0,0,0,0,0],\n                           [0,0,0,1,0,0,0,0],\n                           [0,0,0,0,1,0,0,0],\n                           [0,0,0,0,0,1,0,0],\n                           [0,0,0,0,0,0,1,0],\n                           [0,0,0,0,0,0,0,1]\n                          ])\n\n#              01234567\nclasslabels = 'QRSUVABC'\n\nclasslabels_onehot = np.array([[1,0,0,0,0,0,0,0],\n                               [0,1,0,0,0,0,0,0],\n                               [0,0,1,0,0,0,0,0],\n                               [0,0,0,1,0,0,0,0],\n                               [0,0,0,0,1,0,0,0],\n                               [0,0,0,0,0,1,0,0],\n                               [0,0,0,0,0,0,1,0],\n                               [0,0,0,0,0,0,0,1]\n                              ])\n\n#                            Q       R       S       U       V       A       B      C\nclassidx2rule = np.array([[6,6,6],[6,6,7],[6,7,6],[6,7,7],[7,6,6],[7,6,7],[7,7,6],[7,7,7]\n                         ])"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": "def generate_sequence():\n    seq_length = np.random.choice(range(100, 110))\n    t1 = np.random.choice(range(10,21))\n    t2 = np.random.choice(range(33,44))\n    t3 = np.random.choice(range(66,76)) \n    targetclassidx = np.random.choice(range(0,8)) #randomly choose a target class\n    \n    tagetclass_onehot = classlabels_onehot[targetclassidx]\n    \n    seq = np.zeros((seq_length,1),dtype=\"int\")\n    seq[0] = 0 #first char is E\n    seq[-1] = 1 #last char is B\n    \n    #randomly asaign abcd to the rest of the positions\n    for i in range(1,seq_length):\n        seq[i] = np.random.choice([2,3,4,5])\n\n    # insert X,Y values based on class\n    seq[t1], seq[t2], seq[t3] = classidx2rule[targetclassidx]\n    \n    #generate onehot for sequence\n    seq_onehot = np.zeros((seq_length,8))\n    for idx in range(seq_length):\n        seq_onehot[idx] = symbols_onehot[seq[idx]]\n        \n    return seq_length, seq, seq_onehot, targetclassidx, tagetclass_onehot"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": "class Mylstm(nn.Module):\n    \n    def __init__(self):\n        super(Mylstm, self).__init__()\n        \n        self.lstm1 = nn.LSTM(input_size = 8, hidden_size = 2)\n        self.lstm2 = nn.LSTM(input_size = 2, hidden_size = 4)\n        self.lstm3 = nn.LSTM(input_size = 4, hidden_size = 8)       \n        self.linear = nn.Linear(in_features=8, out_features=8)\n        \n    def forward(self, input):\n       \n        lstm_out1, (self.h1, self.c1) = self.lstm1(input)\n        lstm_out2, (self.h2, self.c2) = self.lstm2(self.h1)\n        lstm_out3, (self.h3, self.c3) = self.lstm3(self.h2)\n           \n        #pred_vec = lstm_out3[-1]\n        pred_vec = self.linear(lstm_out3[-1])\n            \n        return pred_vec\n    \n    def reset_hidden_states(self):\n        (self.h1, self.c1) = (torch.zeros(1, 1, 2), torch.zeros(1, 1, 2))\n        (self.h2, self.c2) = (torch.zeros(1, 1, 4), torch.zeros(1, 1, 4))\n        (self.h3, self.c3) = (torch.zeros(1, 1, 8), torch.zeros(1, 1, 8))\n    "
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "------------------------------------------------------\n0 tensor(1.9732, grad_fn=<NllLossBackward>) tensor([[ 0.1996,  0.1573,  0.3047, -0.2653, -0.2880,  0.3218,  0.1803, -0.0601]],\n       grad_fn=<AddmmBackward>) tensor([0.3218]) tensor([5]) tensor([0]) 0\n------------------------------------------------------\n5000 tensor(2.4131, grad_fn=<NllLossBackward>) tensor([[-0.5152, -0.4003, -0.2986, -0.1385, -0.2899, -0.2808, -0.9371, -0.7558]],\n       grad_fn=<AddmmBackward>) tensor([-0.1385]) tensor([3]) tensor([7]) 614\n------------------------------------------------------\n10000 tensor(2.2653, grad_fn=<NllLossBackward>) tensor([[ 0.1074, -0.5009, -0.5208, -0.2780, -0.3176, -0.8678, -0.9442, -0.6280]],\n       grad_fn=<AddmmBackward>) tensor([0.1074]) tensor([0]) tensor([7]) 588\n------------------------------------------------------\n15000 tensor(1.7375, grad_fn=<NllLossBackward>) tensor([[-0.1147, -0.9970, -0.6853, -0.6455, -0.1148, -0.6777, -0.3246, -0.4221]],\n       grad_fn=<AddmmBackward>) tensor([-0.1147]) tensor([0]) tensor([4]) 632\n------------------------------------------------------\n20000 tensor(2.2453, grad_fn=<NllLossBackward>) tensor([[-0.2356, -0.3213, -0.9903, -0.0408, -0.6107, -0.6305, -0.7025, -0.3428]],\n       grad_fn=<AddmmBackward>) tensor([-0.0408]) tensor([3]) tensor([4]) 646\n------------------------------------------------------\n25000 tensor(2.4873, grad_fn=<NllLossBackward>) tensor([[-0.0459, -0.4273, -0.8311, -0.2083, -0.7210, -0.6360, -0.8501, -0.0602]],\n       grad_fn=<AddmmBackward>) tensor([-0.0459]) tensor([0]) tensor([2]) 624\n------------------------------------------------------\n30000 tensor(2.1002, grad_fn=<NllLossBackward>) tensor([[-0.3360, -0.4653,  0.1296, -1.1031, -0.4503, -0.4599, -0.6116, -0.6839]],\n       grad_fn=<AddmmBackward>) tensor([0.1296]) tensor([2]) tensor([1]) 605\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-03a4401b9d44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMylstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-03a4401b9d44>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": "def train_model(model):\n        \n    loss_fn  = nn.CrossEntropyLoss()\n    \n    optimiser = optim.Adam(model.parameters(), lr = 0.1)\n    \n    count = 0\n    for i in range(1000000):\n        \n        #model.reset_hidden_states()\n        \n        with torch.no_grad():\n            seq_length, seq, seq_onehot, targetclassidx, targetclass_onehot = generate_sequence()\n        \n            seq_onehot = torch.from_numpy(seq_onehot).float()\n        \n            seq_onehot = seq_onehot.view([seq_length,1,8])\n            \n            targetclassidx = torch.tensor([targetclassidx])\n              \n        \n        pred = model(seq_onehot)       \n               \n        loss = loss_fn(pred, targetclassidx)\n        \n        \n        with torch.no_grad():\n            predvalue, predclassidx = torch.max(pred,-1)\n                       \n            if predclassidx == targetclassidx:\n                count = count+1\n                \n            \n            if i % 5000 == 0:\n                print(\"------------------------------------------------------\")\n                print(i,loss, pred, predvalue, predclassidx, targetclassidx, count)\n                count = 0\n                    \n        optimiser.zero_grad()\n        \n        loss.backward()\n        \n        optimiser.step()\n        \n    return model.eval()\n\nmodel1 = Mylstm()\nmodel1 = train_model(model1)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
