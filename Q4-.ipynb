{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nfrom torch import optim\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": "#          01234567\nsymbols = \"EBabcdXY\"\nsymbols_onehot = np.array([[1,0,0,0,0,0,0,0],\n                           [0,1,0,0,0,0,0,0],\n                           [0,0,1,0,0,0,0,0],\n                           [0,0,0,1,0,0,0,0],\n                           [0,0,0,0,1,0,0,0],\n                           [0,0,0,0,0,1,0,0],\n                           [0,0,0,0,0,0,1,0],\n                           [0,0,0,0,0,0,0,1]\n                          ])\n\n#              01234567\nclasslabels = 'QRSUVABC'\n\nclasslabels_onehot = np.array([[1,0,0,0,0,0,0,0],\n                               [0,1,0,0,0,0,0,0],\n                               [0,0,1,0,0,0,0,0],\n                               [0,0,0,1,0,0,0,0],\n                               [0,0,0,0,1,0,0,0],\n                               [0,0,0,0,0,1,0,0],\n                               [0,0,0,0,0,0,1,0],\n                               [0,0,0,0,0,0,0,1]\n                              ])\n\n#                            Q       R       S       U       V       A       B      C\nclassidx2rule = np.array([[6,6,6],[6,6,7],[6,7,6],[6,7,7],[7,6,6],[7,6,7],[7,7,6],[7,7,7]\n                         ])"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": "def generate_sequence():\n    seq_length = np.random.choice(range(100, 110))\n    t1 = np.random.choice(range(10,21))\n    t2 = np.random.choice(range(33,44))\n    t3 = np.random.choice(range(66,76)) \n    targetclassidx = np.random.choice(range(0,8)) #randomly choose a target class\n    \n    tagetclass_onehot = classlabels_onehot[targetclassidx]\n    \n    seq = np.zeros((seq_length,1),dtype=\"int\")\n    seq[0] = 0 #first char is E\n    seq[-1] = 1 #last char is B\n    \n    #randomly asaign abcd to the rest of the positions\n    for i in range(1,seq_length):\n        seq[i] = np.random.choice([2,3,4,5])\n\n    # insert X,Y values based on class\n    seq[t1], seq[t2], seq[t3] = classidx2rule[targetclassidx]\n    \n    #generate onehot for sequence\n    seq_onehot = np.zeros((seq_length,8))\n    for idx in range(seq_length):\n        seq_onehot[idx] = symbols_onehot[seq[idx]]\n        \n    return seq_length, seq, seq_onehot, targetclassidx, tagetclass_onehot"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": "class Mylstm(nn.Module):\n    \n    def __init__(self):\n        super(Mylstm, self).__init__()\n        \n        self.lstm1 = nn.LSTM(input_size = 8, hidden_size = 30, num_layers=3)\n        #self.lstm2 = nn.LSTM(input_size = 2, hidden_size = 4)\n        #self.lstm3 = nn.LSTM(input_size = 4, hidden_size = 8)       \n        self.linear = nn.Linear(in_features=30, out_features=8)\n        \n    def forward(self, input):\n       \n        lstm_out1, (self.h1, self.c1) = self.lstm1(input)\n        #lstm_out2, (self.h2, self.c2) = self.lstm2(self.h1)\n        #lstm_out3, (self.h3, self.c3) = self.lstm3(self.h2)\n           \n        #pred_vec = lstm_out3[-1]\n        pred_vec = self.linear(lstm_out1[-1])\n            \n        return pred_vec\n    \n    def reset_hidden_states(self):\n        (self.h1, self.c1) = (torch.zeros(1, 1, 30), torch.zeros(1, 1, 30))\n        #(self.h2, self.c2) = (torch.zeros(1, 1, 4), torch.zeros(1, 1, 4))\n        #(self.h3, self.c3) = (torch.zeros(1, 1, 8), torch.zeros(1, 1, 8))\n    "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "------------------------------------------------------\n0 tensor(2.1174, grad_fn=<NllLossBackward>) tensor([[ 0.0831,  0.0164,  0.1731,  0.0320, -0.0128, -0.1259,  0.0894,  0.1483]],\n       grad_fn=<AddmmBackward>) tensor([0.1731]) tensor([2]) tensor([1]) 0\n------------------------------------------------------\n5000 tensor(2.0260, grad_fn=<NllLossBackward>) tensor([[-0.2236, -0.1748, -0.6512,  0.1268, -0.1441, -0.4830, -0.2735, -0.1882]],\n       grad_fn=<AddmmBackward>) tensor([0.1268]) tensor([3]) tensor([1]) 603\n------------------------------------------------------\n10000 tensor(2.3387, grad_fn=<NllLossBackward>) tensor([[-0.4447, -0.4116,  0.3601, -0.9638, -0.0708, -0.3844, -0.6763,  0.5987]],\n       grad_fn=<AddmmBackward>) tensor([0.5987]) tensor([7]) tensor([5]) 635\n------------------------------------------------------\n15000 tensor(2.1495, grad_fn=<NllLossBackward>) tensor([[ 0.3413, -0.4657, -0.2559, -0.1735, -0.7296,  0.0045,  0.0116, -0.7019]],\n       grad_fn=<AddmmBackward>) tensor([0.3413]) tensor([0]) tensor([2]) 669\n------------------------------------------------------\n20000 tensor(2.1525, grad_fn=<NllLossBackward>) tensor([[ 0.6453, -0.3308, -0.0519, -0.3908, -0.2516, -0.6009, -0.7705, -0.2307]],\n       grad_fn=<AddmmBackward>) tensor([0.6453]) tensor([0]) tensor([7]) 618\n------------------------------------------------------\n25000 tensor(2.3791, grad_fn=<NllLossBackward>) tensor([[-0.4688, -0.2106, -0.5730, -0.0171, -0.2658,  0.3862, -0.4980, -0.3118]],\n       grad_fn=<AddmmBackward>) tensor([0.3862]) tensor([5]) tensor([6]) 629\n------------------------------------------------------\n30000 tensor(2.6136, grad_fn=<NllLossBackward>) tensor([[-0.7468, -0.2377, -0.2996, -0.4296,  0.1158, -0.5244,  0.1831, -0.1077]],\n       grad_fn=<AddmmBackward>) tensor([0.1831]) tensor([6]) tensor([0]) 623\n------------------------------------------------------\n35000 tensor(2.1083, grad_fn=<NllLossBackward>) tensor([[-0.2600, -0.4807,  0.2753, -0.2093, -0.1966, -0.3876, -0.4524, -0.3638]],\n       grad_fn=<AddmmBackward>) tensor([0.2753]) tensor([2]) tensor([0]) 629\n------------------------------------------------------\n40000 tensor(2.7725, grad_fn=<NllLossBackward>) tensor([[-0.8851, -0.1521, -0.4634, -0.4873,  0.0026, -0.1160, -0.7318,  0.5384]],\n       grad_fn=<AddmmBackward>) tensor([0.5384]) tensor([7]) tensor([0]) 620\n------------------------------------------------------\n45000 tensor(2.0517, grad_fn=<NllLossBackward>) tensor([[-0.2706, -0.2684, -0.1931, -0.0842, -0.3435, -0.0735,  0.1987, -1.2529]],\n       grad_fn=<AddmmBackward>) tensor([0.1987]) tensor([6]) tensor([2]) 652\n------------------------------------------------------\n50000 tensor(2.1999, grad_fn=<NllLossBackward>) tensor([[-0.8976, -0.3157, -0.1707, -0.5527, -0.4773,  0.1215,  0.3612, -0.1734]],\n       grad_fn=<AddmmBackward>) tensor([0.3612]) tensor([6]) tensor([1]) 654\n------------------------------------------------------\n55000 tensor(2.5028, grad_fn=<NllLossBackward>) tensor([[-1.1087, -0.5933,  0.1254, -0.1780, -0.0496,  0.0152, -0.5102,  0.2847]],\n       grad_fn=<AddmmBackward>) tensor([0.2847]) tensor([7]) tensor([1]) 641\n"
    }
   ],
   "source": "def train_model(model):\n        \n    loss_fn  = nn.CrossEntropyLoss()\n    \n    optimiser = optim.Adam(model.parameters(), lr = 0.1)\n    \n    count = 0\n    for i in range(1000000):\n        \n        #model.reset_hidden_states()\n        \n        with torch.no_grad():\n            seq_length, seq, seq_onehot, targetclassidx, targetclass_onehot = generate_sequence()\n        \n            seq_onehot = torch.from_numpy(seq_onehot).float()\n        \n            seq_onehot = seq_onehot.view([seq_length,1,8])\n            \n            targetclassidx = torch.tensor([targetclassidx])\n              \n        \n        pred = model(seq_onehot)       \n               \n        loss = loss_fn(pred, targetclassidx)\n        \n        \n        with torch.no_grad():\n            predvalue, predclassidx = torch.max(pred,-1)\n                       \n            if predclassidx == targetclassidx:\n                count = count+1\n                \n            \n            if i % 5000 == 0:\n                print(\"------------------------------------------------------\")\n                print(i,loss, pred, predvalue, predclassidx, targetclassidx, count)\n                count = 0\n                    \n        optimiser.zero_grad()\n        \n        loss.backward()\n        \n        optimiser.step()\n        \n    return model.eval()\n\nmodel1 = Mylstm()\nmodel1 = train_model(model1)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
